{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4613042a",
   "metadata": {},
   "source": [
    "# Week 3 Tasks Sub-list 3\n",
    "Student Name: Zhangli Wang  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4677f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import copy\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba41f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "df = pd.read_csv('./Datasets/rawpvr_2018-02-01_28d_1083 TueFri.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927a009",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "Read the original csv file 'rawpvr_2018-02-01_28d_1083 TueFri.csv' into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e7e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice to acquire the 'Date' column\n",
    "date = df.loc[:, 'Date']\n",
    "# type conversion: string > timestamp\n",
    "date = pd.to_datetime(date)\n",
    "# calculate days of the week\n",
    "days_of_week = [item.dayofweek for item in date]\n",
    "# type conversion: list > series\n",
    "days_of_week = pd.Series(np.array(days_of_week).T)\n",
    "# update the column 'Flags' in the original dataframe\n",
    "df.loc[:, 'Flags'] = days_of_week.map(lambda x: x+1, na_action='ignore')\n",
    "# update the column 'Flag Text' in the original dataframe\n",
    "df.loc[:, 'Flag Text'] = days_of_week.map(lambda x: 'Tuesday' if x == 1 else ('Friday' if x == 4 else ''), na_action='ignore')\n",
    "# add a new column 'Time' to store the time of the date\n",
    "df = df.assign(Time = pd.to_datetime(df['Date']).dt.time)\n",
    "# add a new column 'DateWOTime' to store the date without the time\n",
    "df = df.assign(DateWOTime = pd.to_datetime(df['Date']).dt.date)\n",
    "# transform the type of data in the 'Date' column of the dataset: string > timestamp\n",
    "df = df.assign(Date = pd.to_datetime(df['Date']))\n",
    "# add a new column 'DateHour' to store the date in the format of '%year%-%month%-%day% %hour%:00:00'\n",
    "df['DateHour'] = df['Date'].apply(lambda x: datetime.datetime(x.year, x.month, x.day, x.hour, 0, 0))\n",
    "# add a new column 'Hour' to store the hour in the format of '%hour%:00:00'\n",
    "df['Hour'] = df['Date'].apply(lambda x: datetime.time(x.hour, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f760477",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is the data preparation operations for the input dataset. \n",
    "- Update the column `Flag` and `Flag Text` with the day of the week id and the day of the week, respectively.  \n",
    "    - Create a series of the days of week.  \n",
    "    ```python\n",
    "    # slice to acquire the 'Date' column\n",
    "    date = df.loc[:, 'Date']\n",
    "    # type conversion: string > timestamp\n",
    "    date = pd.to_datetime(date)\n",
    "    # calculate days of the week\n",
    "    days_of_week = [item.dayofweek for item in date]\n",
    "    # type conversion: list > series\n",
    "    days_of_week = pd.Series(np.array(days_of_week).T)\n",
    "    ```\n",
    "    Firstly, the column `Date` is sliced from the original dataframe and converted to the type timestamp. Then the days of week is calculated and its type is converted from list to series for adding columns in the dataframe.  \n",
    "    - Update the columns `Flag` and `Flag Text`.  \n",
    "    ```python\n",
    "    # update the column 'Flags' in the original dataframe\n",
    "    df.loc[:, 'Flags'] = days_of_week.map(lambda x: x+1, na_action='ignore')\n",
    "    # update the column 'Flag Text' in the original dataframe\n",
    "    df.loc[:, 'Flag Text'] = days_of_week.map(lambda x: 'Tuesday' if x == 1 else ('Friday' if x == 4 else ''), na_action='ignore')\n",
    "    ```\n",
    "    Use map functions to map the series to the new columns in the dataframe with specified requirements.\n",
    "- Add a new column `Time` in the dataframe to store the time in dates\n",
    "```python\n",
    "# add a new column `time` in the dataframe to store the time of the date\n",
    "df = df.assign(Time = pd.to_datetime(df['Date']).dt.time)\n",
    "```\n",
    "- Add a new 'DateWOTime' to store the date without the time\n",
    "```python\n",
    "# add a new column `DateWOTime` in the dataframe to store the date without the time\n",
    "df = df.assign(DateWOTime = pd.to_datetime(df['Date']).dt.date)\n",
    "```\n",
    "- Change the datatype of `Date` from `string` to `timestamp`\n",
    "```python\n",
    "# transform the type of data in the 'Date' column of the dataset: string > timestamp\n",
    "df = df.assign(Date = pd.to_datetime(df['Date']))\n",
    "```\n",
    "This operation is used for filtering the dataframe according to dates in the future and is a necessary step for continuing the very next data preparasion operasion.\n",
    "- Add a new column `DateHour` to store the date in the format of '%year%-%month%-%day% %hour%:00:00'\n",
    "```python\n",
    "# add a new column 'DateHour' to store the date in the format of '%year%-%month%-%day% %hour%:00:00'\n",
    "df['DateHour'] = df['Date'].apply(lambda x: datetime.datetime(x.year, x.month, x.day, x.hour, 0, 0))\n",
    "```\n",
    "This operation is useful for filtering the dataframe according to one-hour time intervals.  \n",
    "  \n",
    "The data preparasion operations mainly focus on altering and adding variaty of the time attribute for the convenience of filtering the dataframe accordign to time. In the future, for each sub-task, more data preparasion operations are executed and they mainly focus on filtering to a new dataframe which is to meet the constraints for achieving the final goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ebe5b",
   "metadata": {},
   "source": [
    "## Task5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c072b",
   "metadata": {},
   "source": [
    "### Task5.1\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabbd2f",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac77120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataset with regard to 'Flag Text'\n",
    "df1 = df.loc[df['Flag Text'] == 'Tuesday']\n",
    "# filter the dataset with regard to 'Time'\n",
    "start_time = datetime.time(7, 0, 0)\n",
    "end_time = datetime.time(19, 0, 0)\n",
    "df1 = df1.loc[(df1['Time'] > start_time) & (df1['Time'] < end_time)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a9fe0",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is the data preparation operations for Task5.1 that filters the dataframe to a new dataframe `df1` according to constraints of the dataset in Task5.1.\n",
    "- Filter the dataset with regard to `Flag Text` that the day of the week is Tuesday.\n",
    "```python\n",
    "# filter the dataset with regard to 'Flag Text'\n",
    "df1 = df.loc[df['Flag Text'] == 'Tuesday']\n",
    "```\n",
    "- Filter the dataframe with regard to `Time` that the time is between 7:00 and 19:00.\n",
    "```python\n",
    "# filter the dataset with regard to 'Time'\n",
    "start_time = datetime.time(7, 0, 0)\n",
    "end_time = datetime.time(19, 0, 0)\n",
    "df1 = df.loc[(df['Time'] > start_time) & (df['Time'] < end_time)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f23ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column completeness gap: 98.03654443753885\n",
      "column completeness headway: 98.96532007458049\n"
     ]
    }
   ],
   "source": [
    "num_cells = len(df1)  # number of cells\n",
    "\n",
    "# number of non-empty cells of the column 'Gap (s)'\n",
    "num_non_e_cells_gap = len(df1['Gap (s)'].notna()[df1['Gap (s)'].notna() == True]) \n",
    "# column completeness of the column 'Gap (s)'\n",
    "column_completeness_gap = 100 * num_non_e_cells_gap / num_cells\n",
    "\n",
    "# number of non-empty cells of the column 'Headway (s)'\n",
    "num_non_e_cells_headway = len(df1['Headway (s)'].notna()[df1['Headway (s)'].notna() == True])\n",
    "# column comleteness of the column 'Headway (s)'\n",
    "column_completeness_headway = 100 * num_non_e_cells_headway / num_cells\n",
    "\n",
    "print('column completeness gap: ' + str(column_completeness_gap))\n",
    "print('column completeness headway: ' + str(column_completeness_headway))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042d691",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is calculation of column completeness for the columns `Gap (s)` and `Headway (s)`. Firstly, the length of the columns in `df1` are calculated by `num_cells = len(df1)`. Then, the number of non-empty cells of each column are calculated like\n",
    "```python\n",
    "# number of non-empty cells of the column 'Gap (s)'\n",
    "num_non_e_cells_gap = len(df1['Gap (s)'].notna()[df1['Gap (s)'].notna() == True]) \n",
    "```\n",
    ". Finally, the the values of column completeness are calculated like\n",
    "```python\n",
    "# column completeness of the column 'Gap (s)'\n",
    "column_completeness_gap = 100 * num_non_e_cells_gap / num_cells\n",
    "```\n",
    ".\n",
    "\n",
    "**Result**:\n",
    "- column completeness gap: 98.03654443753885\n",
    "- column completeness headway: 98.96532007458049  \n",
    "\n",
    "**Interpretation of the result**:\\\n",
    "The completeness values of the columns 'Gap (s)' and 'Headway (s)' are both over 98.0, which means that over 98 percent of the values are not empty. So for both the two columns, most of the records are filled with data. But there still exists a small proportion of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165f7d9",
   "metadata": {},
   "source": [
    "### Task5.2\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d555e",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d8ba5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize dataframe for storing interpolated dataframe\n",
    "df2 = df1.copy(deep=True)\n",
    "# filter with regard to 'Lane Name'\n",
    "df2 = df2.loc[df2['Lane Name'] == 'NB_MID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd79679",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is the data preparation operations for Task5.2. Because the dataset to be used in this task also requires the same constraints as in Task5.1, a new dataframe `df2` is created from the dataframe used in Task5.1 as initialization.\n",
    "```python\n",
    "df2 = df1.copy(deep=True)\n",
    "```\n",
    "- Filter the dataframe with regard to `Lane Name` that the lane name is 'NB_MID'\n",
    "```python\n",
    "# filter with regard to 'Lane Name'\n",
    "df2 = df2.loc[df2['Lane Name'] == 'NB_MID']\n",
    "```\n",
    "\n",
    "In conclusion, now the dataframe has additional restrictions that the `Flag Text` is 'Tuesday', the `Time` is ranged from 7:00 to 19:00, and the `Lane Name` is 'NB_MID'. In the next code block, data manipulation on `df2` can be done with the preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb954baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = dict()  # key: column name, value: medians of the column\n",
    "for column_name in ['Gap (s)', 'Headway (s)']:\n",
    "    # record unique hour\n",
    "    unique_hours = df2['Hour'].unique()\n",
    "    \n",
    "    # calculate median values for all the time intervals\n",
    "    medians_col = dict()  # key: hour, value: median value\n",
    "    for unique_hour in unique_hours:\n",
    "        # filter the dataframe with regard to date hour\n",
    "        filtered_df2 = df2.loc[df2['Hour'] == unique_hour]\n",
    "        \n",
    "        # get the values of the column\n",
    "        values = filtered_df2[column_name]\n",
    "        \n",
    "        # record the median\n",
    "        medians_col.update({unique_hour: values.median()})\n",
    "    \n",
    "    # record the medians of the column\n",
    "    medians.update({column_name: medians_col})\n",
    "        \n",
    "    # find the series of 'Date' of dataframe if the column_name is null\n",
    "    ss_date_missing = df2.loc[df2[column_name].isna()]['Date']\n",
    "    \n",
    "    # update the cell of the dataframe with the median value\n",
    "    for item in ss_date_missing:\n",
    "        # get the row of the dataframe where the value is missing\n",
    "        tempdf = df2.loc[df2['Date'] == item]\n",
    "\n",
    "        # get the corresponding median value\n",
    "        median_value = medians[column_name].get(tempdf['Hour'].iloc[0])\n",
    "    \n",
    "        # interpolate value to the dataframe\n",
    "        df2.loc[df2['Date'] == item, column_name] = median_value\n",
    "\n",
    "# print results\n",
    "# print(medians)\n",
    "\n",
    "# clear memory\n",
    "# del df1\n",
    "# del df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5464f0f3",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is the data manipulation on `df2`. Overall, there are two processes explained below.\n",
    "- Find median values of the columns `Gap (s)` and `Headway (s)` for all one-hour intervals.\n",
    "    - Firstly we initialize the object to store all the median values. It is a dictionary storing names of the columns `Gap (s)` and `Headway (s)` as two keys. And their values are dictionaries that stores median values of corresponding columns with regard to one-hour time intervals in the day.\n",
    "    ```python\n",
    "    medians = dict()  # key: column name, value: medians of the column\n",
    "    ```\n",
    "    - Then we iterate over the two columns `Gap (s)` and `Headway (s)` to execute the same operations to them. Following mentioned operations are executed in this iteration.\n",
    "    ```python\n",
    "    for column_name in ['Gap (s)', 'Headway (s)']:\n",
    "    ```\n",
    "    - To acheive the goal, we may want to iteratively filter `df2` according to each one-hour time interval between 7:00 and 19:00 and calculate median values. So to begin with we find all unique one-hour time intervals in `df2`.\n",
    "    ```python\n",
    "    # record unique hour\n",
    "    unique_hours = df2['Hour'].unique()\n",
    "    ```\n",
    "    - Then we execute operations mentioned in the previous bullet point and the median values for the column are calculated and updated to `medians`.\n",
    "    ```python\n",
    "    # calculate median values for all the time intervals\n",
    "    medians_col = dict()  # key: date hour, value: median value\n",
    "    for unique_hour in unique_hours:\n",
    "        # filter the dataframe with regard to hour\n",
    "        filtered_df2 = df2.loc[df2['Hour'] == unique_hour]\n",
    "        \n",
    "        # get the values of the column\n",
    "        values = filtered_df2[column_name]\n",
    "        \n",
    "        # record the median\n",
    "        medians_col.update({unique_hour: values.median()})\n",
    "        \n",
    "    # record the medians of the column\n",
    "    medians.update({column_name: medians_col})\n",
    "    ```\n",
    "- Update the cells with missing values with the corresponding median values.\n",
    "    - To acheive this goal, we need to find and record all the rows that have missing values. We can only take the column `Date` for future comparison as the dates are unique.\n",
    "    ```python\n",
    "    # find the series of 'Date' of dataframe if the column_name is null\n",
    "    ss_date_missing = df2.loc[df2[column_name].isna()]['Date']\n",
    "    ```\n",
    "    - Now we can update the values by iteratively find a missing row in `df2` and update it with the corresponding value.\n",
    "    ```python\n",
    "    # update the cell of the dataframe with the median value\n",
    "    for item in ss_date_missing:\n",
    "        # get the row of the dataframe where the value is missing\n",
    "        tempdf = df2.loc[df2['Date'] == item]\n",
    "\n",
    "        # get the corresponding median value\n",
    "        median_value = medians[column_name].get(tempdf['Hour'].iloc[0])\n",
    "    \n",
    "        # interpolate value to the dataframe\n",
    "        df2.loc[df2['Date'] == item, column_name] = median_value\n",
    "    ```\n",
    "    In the iteration, the first step is to get the row that the date (unique key) in `df2` is matched with the one in `ss_date_missing`. The second step is to find the corresponding median value in `medians` according to the time of the row. The final step is updating the row.\n",
    "\n",
    "**Result of median values**:\n",
    "```python\n",
    "{'Gap (s)': \n",
    "  {datetime.time(7, 0): 1.834,\n",
    "  datetime.time(8, 0): 2.1020000000000003,\n",
    "  datetime.time(9, 0): 2.08,\n",
    "  datetime.time(10, 0): 2.5835,\n",
    "  datetime.time(11, 0): 2.7785,\n",
    "  datetime.time(12, 0): 2.795,\n",
    "  datetime.time(13, 0): 2.7560000000000002,\n",
    "  datetime.time(14, 0): 2.8049999999999997,\n",
    "  datetime.time(15, 0): 2.577,\n",
    "  datetime.time(16, 0): 2.3225,\n",
    "  datetime.time(17, 0): 2.187,\n",
    "  datetime.time(18, 0): 2.228},\n",
    " 'Headway (s)': \n",
    "  {datetime.time(7, 0): 2.722,\n",
    "  datetime.time(8, 0): 3.1910000000000003,\n",
    "  datetime.time(9, 0): 2.72,\n",
    "  datetime.time(10, 0): 3.0,\n",
    "  datetime.time(11, 0): 3.21,\n",
    "  datetime.time(12, 0): 3.16,\n",
    "  datetime.time(13, 0): 3.102,\n",
    "  datetime.time(14, 0): 3.133,\n",
    "  datetime.time(15, 0): 2.92,\n",
    "  datetime.time(16, 0): 2.853,\n",
    "  datetime.time(17, 0): 2.9065,\n",
    "  datetime.time(18, 0): 2.79}}\n",
    "```\n",
    "**Result of screenshot of the updated dataframe**:\n",
    "![img](./images/img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17daa469",
   "metadata": {},
   "source": [
    "## Task6\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd940e3",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5403fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset of the site 1415\n",
    "dfb = pd.read_csv('./Datasets/rawpvr_2018-02-01_28d_1415 TueFri.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e610e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the same data preparation executions to dataframe b\n",
    "\n",
    "# slice to acquire the 'Date' column\n",
    "date = dfb.loc[:, 'Date']\n",
    "# type conversion: string > timestamp\n",
    "date = pd.to_datetime(date)\n",
    "# calculate days of the week\n",
    "days_of_week = [item.dayofweek for item in date]\n",
    "# type conversion: list > series\n",
    "days_of_week = pd.Series(np.array(days_of_week).T)\n",
    "# update the column 'Flags' in the original dataframe\n",
    "dfb.loc[:, 'Flags'] = days_of_week.map(lambda x: x+1, na_action='ignore')\n",
    "# update the column 'Flag Text' in the original dataframe\n",
    "dfb.loc[:, 'Flag Text'] = days_of_week.map(lambda x: 'Tuesday' if x == 1 else ('Friday' if x == 4 else ''), na_action='ignore')\n",
    "# add a new column 'Time' to store the time of the date\n",
    "dfb = dfb.assign(Time = pd.to_datetime(dfb['Date']).dt.time)\n",
    "# add a new column 'DateWOTime' to store the date without the time\n",
    "dfb = dfb.assign(DateWOTime = pd.to_datetime(dfb['Date']).dt.date)\n",
    "# transform the type of data in the 'Date' column of the dataset: string > timestamp\n",
    "dfb = dfb.assign(Date = pd.to_datetime(dfb['Date']))\n",
    "# add a new column 'DateHour' to store the date in the format of '%year%-%month%-%day% %hour%:00:00'\n",
    "dfb['DateHour'] = dfb['Date'].apply(lambda x: datetime.datetime(x.year, x.month, x.day, x.hour, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c213b2",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "In this code block, we simply execute the same data preparation operations for `dfb` as the ones we did for `df` for consistency and future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b09a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a alias for the dataframe of the site 1083 for this task\n",
    "dfa = df\n",
    "# use a variable to reference the two dataframes\n",
    "dfs = {'1083': dfa, \n",
    "     '1415': dfb}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c325185",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "Firstly an alias of the dataframe for the site 1083 is created (`dfa`), this is for standardize the names for the dataframes in avoidance of generating potential inconsistency in the future. Then the dataframes are referenced together in one variable (`dfs`) for the convenience of coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37d21083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the two dataframes\n",
    "\n",
    "# create a copy of the old dataframes\n",
    "dfs1 = copy.deepcopy(dfs)\n",
    "for key, df in dfs1.items():\n",
    "    # filter the dataframe with regard to 'Time'\n",
    "    start_time = datetime.time(17, 0, 0)\n",
    "    end_time = datetime.time(18, 0, 0)\n",
    "    df = df.loc[(df['Time'] > start_time) & (df['Time'] < end_time)]\n",
    "    \n",
    "    # filter the dataframe with regard to 'Direction'\n",
    "    df = df.loc[df['Direction'] == 1]\n",
    "    \n",
    "    # filter the dataframe with regard to 'Flag Text'\n",
    "    df = df.loc[df['Flag Text'] == 'Friday']\n",
    "    \n",
    "    # record\n",
    "    dfs1[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360482b3",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is the data preparation operations for Task6. Because the operations are specificly focused on Task6, we created a new version `dfs1` of the dataframe `dfs` similar to the things we did before `dfs1 = copy.deepcopy(dfs)`.\\\n",
    "\n",
    "Then we iterate on `dfs1` for each dataframe and perform the following operations.\n",
    "- Filter the dataframe with regard to 'Time' that the time is between 17:00 and 18:00.\n",
    "```python\n",
    "# filter the dataframe with regard to 'Time'\n",
    "start_time = datetime.time(17, 0, 0)\n",
    "end_time = datetime.time(18, 0, 0)\n",
    "df = df.loc[(df['Time'] > start_time) & (df['Time'] < end_time)]\n",
    "```\n",
    "- Filter the dataframe with regard to 'Direction' that the direction is heading north.\n",
    "```python\n",
    "# filter the dataframe with regard to 'Direction'\n",
    "df = df.loc[df['Direction'] == 1]\n",
    "```\n",
    "- Filter the dataframe with regard to 'Flag Text' that the day of the week is Friday.\n",
    "```python\n",
    "# filter the dataframe with regard to 'Flag Text'\n",
    "df = df.loc[df['Flag Text'] == 'Friday']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5038535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record unique lane names\n",
    "unique_lane_names = dict()\n",
    "for key, df in dfs1.items():\n",
    "    # find unique lane names for the dataframe\n",
    "    unique_lane_name = df['Lane Name'].unique()\n",
    "    \n",
    "    # record\n",
    "    unique_lane_names.update({key: unique_lane_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5f6a0",
   "metadata": {},
   "source": [
    "*Explanation*:\n",
    "This code block is an intermediate process to record unique lane names `unique_lane_names` of the two dataframes for the usage in the next code block.\\\n",
    "**Result of `unique_lane_names`**:\n",
    "```python\n",
    "{'1083': array(['NB_OS', 'NB_MID', 'NB_NS'], dtype=object),\n",
    " '1415': array(['NE_NS', 'NE_OS'], dtype=object)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e06eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1083': [6.009038525021391, 6.295469340374707, 7.279743256362566], '1415': [7.519402241603255, 6.6714418953907915]}\n",
      "6.7550190517505415\n"
     ]
    }
   ],
   "source": [
    "# find average travel time of the lanes\n",
    "avg_times_lanes = dict()  # the average travel times of the two dataframes\n",
    "for key, df in dfs1.items():\n",
    "    avg_times_lanes_df = list()  # the average speeds of the dataframe\n",
    "    for lane_name in unique_lane_names[key]:\n",
    "        # filter the dataframe with regard to 'Lane Name'\n",
    "        filtered_df = df.loc[df['Lane Name'] == lane_name]\n",
    "        \n",
    "        # calculate the average speed\n",
    "        avg_speed = filtered_df['Speed (mph)'].mean()\n",
    "        \n",
    "        # calculate the average travel time in minute by the equation 'time = distance/speed'\n",
    "        #  4.86km = 3.019864miles\n",
    "        avg_time_lane = 3.019864 / avg_speed * 60\n",
    "        \n",
    "        # record\n",
    "        avg_times_lanes_df.append(avg_time_lane)\n",
    "    avg_times_lanes.update({key: avg_times_lanes_df})\n",
    "\n",
    "# print result of average speeds in different lanes\n",
    "print(avg_times_lanes)\n",
    "\n",
    "# calculate the average value of the result\n",
    "templ = list(map(lambda l: (len(l), sum(l)), avg_times_lanes.values()))\n",
    "sum_templ = tuple(map(sum, zip(*templ)))  # * is for unpacking the list  cool\n",
    "avg_time = sum_templ[1]/sum_templ[0]\n",
    "\n",
    "# print result of average time\n",
    "print(avg_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a9354",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is about finding the average travel times in different lanes and calculating their average value. The two processes are introduced below. \n",
    "- Find the average travel times in different lanes.\n",
    "    - To begin with, a dictionary `avg_times_lanes` is created `avg_times_lanes = dict()` to store the name of the sites as the keys and their corresponding average values with regard to different lanes.\n",
    "    - Then the dataframes are iterated. In each iteration, we iteratively filter the dataframe with regard to the lane names.\n",
    "    ```python\n",
    "    for lane_name in unique_lane_names[key]:\n",
    "        # filter the dataframe with regard to 'Lane Name'\n",
    "        filtered_df = df.loc[df['Lane Name'] == lane_name]\n",
    "    ```\n",
    "    - Next, the average speed of the `filtered_df` is calculated `avg_speed = filtered_df['Speed (mph)'].mean()`.\n",
    "    - Next, the average time was calculated by the average speed and the given distance, we firstly changed the unit of the distance from 'km' to 'miles' `avg_time_lane = 3.019864 / avg_speed * 60`.\n",
    "    - Finally we record the results, following is the integral process.\n",
    "    ```python\n",
    "    avg_times_lanes_df = list()  # the average speeds of the dataframe\n",
    "    for lane_name in unique_lane_names[key]:\n",
    "        # filter the dataframe with regard to 'Lane Name'\n",
    "        filtered_df = df.loc[df['Lane Name'] == lane_name]\n",
    "\n",
    "        # calculate the average speed\n",
    "        avg_speed = filtered_df['Speed (mph)'].mean()\n",
    "\n",
    "        # calculate the average travel time in minute by the equation 'time = distance/speed'\n",
    "        #  4.86km = 3.019864miles\n",
    "        avg_time_lane = 3.019864 / avg_speed * 60\n",
    "\n",
    "        # record\n",
    "        avg_times_lanes_df.append(avg_time_lane)\n",
    "    avg_times_lanes.update({key: avg_times_lanes_df})\n",
    "    ```\n",
    "- Calculate the average value of the average travel times.\n",
    "    - For calculating the average value of the result, firstly the number of elements and the sum of the values of the elements are calculated and stored in `templ`.\n",
    "    ```python\n",
    "    templ = list(map(lambda l: (len(l), sum(l)), avg_times_lanes.values()))\n",
    "    ```\n",
    "    ```python\n",
    "    [(3, 19.584251121758662), (2, 14.190844136994047)]\n",
    "    ```\n",
    "    - Then, a map function is used to calculate the sum of values in the tuples in the list and finally calculate the average travel time by dividing the sum of speeds to the number of elements.\n",
    "    ```python\n",
    "    sum_templ = tuple(map(sum, zip(*templ)))  # * is for unpacking the list  cool\n",
    "    avg_time = sum_templ[1]/sum_templ[0]\n",
    "    ```\n",
    "\n",
    "**Results of average travel time in different lanes**:\n",
    "```python\n",
    "{'1083': [6.009038525021391, 6.295469340374707, 7.279743256362566], \n",
    " '1415': [7.519402241603255, 6.6714418953907915]}\n",
    "```\n",
    "**Result of overall average travel time**:\n",
    "- 6.7550190517505415 minutes\n",
    "\n",
    "**Interpretation of the result**:\\\n",
    "The result shows that according to this dataset, the average travel time of north lanes between the two sites in the time from 17:00 to 18:00 in Friday is around 6.755 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5318d",
   "metadata": {},
   "source": [
    "## Task7.1\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f611249",
   "metadata": {},
   "source": [
    "**Solution (i)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143b2a2",
   "metadata": {},
   "source": [
    "I can describe the *Row Completeness* as a set of fractions with regard to combinations of columns that can possibly have missing values, where the numerators are the numbers of rows that are with missing values in certain columns and the denominators are the total number of rows.\n",
    "\n",
    "$$RowCompleteness_1 = \\{RowCompleteness_C: C\\,is\\,the\\,set\\,of\\,all\\,possible\\,combinations\\,of\\,columns\\}$$\n",
    "$$RowCompleteness_{C} = NumberOfMissingRows_{C} \\times 100 \\div NumberOfRows$$\n",
    "\n",
    "The row completeness value indicate the proportion of the number of not-missing values to the total number of values.\n",
    "\n",
    "Besides we can calculate the row completeness of the proportion of the rows that have any missing values to the total number of rows.\n",
    "$$RowCompleteness_2 = NumberOfRowsWithMissingValues \\times 100 \\div NumberOfRows$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bedfa",
   "metadata": {},
   "source": [
    "**Solution (ii)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e36a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe as we did before\n",
    "df3 = df1.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39464190",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is the data preparation operations for Task7.1. Since the filtering requirements in this task are the same as in Task5.1, we can reference to the codes in the Task5.1 section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694b88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the columns that contain missing values\n",
    "ifna_cols = df3.isna().any()\n",
    "missing_cols = list(ifna_cols[ifna_cols == True].keys())\n",
    "\n",
    "# find all possible combinations in the missing columns\n",
    "# reference: https://stackoverflow.com/questions/464864/how-to-get-all-possible-combinations-of-a-list-s-elements?rq=1\n",
    "combs = sum([list(map(list, combinations(missing_cols, i))) for i in range(len(missing_cols) + 1)], [])\n",
    "combs.remove([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde23c41",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "Before we start the manipulations, we should know which columns contain missing values and find all possible combinations of them. As we do not need the empty set as a combination, we simply remove it from our combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54a0f7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize row completeness object\n",
    "RC_count = dict()  # key: combination of columns, value: count\n",
    "\n",
    "for comb in combs:\n",
    "    # filter the dataframe with regard to combination of columns\n",
    "    filtered_df3 = df3[comb]\n",
    "    \n",
    "    # calculate the number of rows that all cells are of missing values\n",
    "    count = len(filtered_df3.loc[filtered_df3.isnull().all(axis=1)].index)\n",
    "    \n",
    "    # record\n",
    "    RC_count.update({str(comb): count})\n",
    "\n",
    "# calculate the row completeness\n",
    "total_count = len(df3.index)\n",
    "RC1 = dict(zip(RC_count.keys(), map(lambda x: 100 * int(x) / total_count, RC_count.values())))\n",
    "\n",
    "# calculate the the row completeness for the entire dataframe\n",
    "count = len(df3.loc[df3.isnull().any(axis=1)].index)\n",
    "RC2 = 100 * count / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825da837",
   "metadata": {},
   "source": [
    "*Explanation*:\\\n",
    "This code block is about calculating the row completeness.\n",
    "- First, we initialize a row completeness count dictionary that stores the combinations of columns as key and the count of the rows that cells in the combination of columns are missing.\n",
    "```python\n",
    "RC_count = dict()  # key: combination of columns, value: count\n",
    "```\n",
    "- Then we iteratively filter `df3` with regard to the combination of columns `filtered_df3 = df3[comb]`, calculate the counts, and record them to `RC_count`.\n",
    "```python\n",
    "for comb in combs:\n",
    "    # filter the dataframe with regard to combination of columns\n",
    "    filtered_df3 = df3[comb]\n",
    "    \n",
    "    # calculate the number of rows that all cells are of missing values\n",
    "    count = len(filtered_df3.loc[filtered_df3.isnull().all(axis=1)].index)\n",
    "    \n",
    "    # record\n",
    "    RC_count.update({str(comb): count})\n",
    "RC_count\n",
    "```\n",
    "- Finally, we need to map `RC_count` according to the previously defined row completeness equation.\n",
    "```python\n",
    "# calculate the row completeness\n",
    "total_count = len(df3.index)\n",
    "RC = dict(zip(RC_count.keys(), map(lambda x: 100 * int(x) / total_count, RC_count.values())))\n",
    "```\n",
    "\n",
    "\n",
    "**Result of Row Completeness Calculation 1**:\n",
    "```python\n",
    "{\"['Speed (mph)']\": 0.004474829086389061,\n",
    " \"['Headway (s)']\": 1.0346799254195151,\n",
    " \"['Gap (s)']\": 1.963455562461156,\n",
    " \"['Speed (mph)', 'Headway (s)']\": 0.0,\n",
    " \"['Speed (mph)', 'Gap (s)']\": 0.0,\n",
    " \"['Headway (s)', 'Gap (s)']\": 1.0346799254195151,\n",
    " \"['Speed (mph)', 'Headway (s)', 'Gap (s)']\": 0.0}\n",
    "```\n",
    "**Result of Row Completeness Calculation 2**:\\\n",
    "1.967930391547545"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526fcca",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "#### Interpretation of the Results\n",
    "The result shows the values of row completeness considering all combinations of columns. Overall, the total rate of rows that have missing values is about 1.96793. For individual columns, the 'Speed (mph)' column has the minimum percentage of missing values, which is around 0.00447. And the column 'Gap (s)' has the highest proportion 1.96345. And we can infer from the result that for all the combinations that have the column 'Speed (mph)' and other columns, all the 'Speed (mph)' columns are filled with values. And in the combination ('Headway (s)', 'Gap (s)'), at least all the values in the column 'Headway (s)' are missing.\n",
    "#### Understanding of Completeness\n",
    "The completeness is an important aspect in data quality. Data completeness can be defined that \"the degree to which a data collection has values or all attributes of entities that are supposed to have values\" [1]. In my interpretation, to see from the perspective of application, the completeness of data is that within the scale of the potential utilization of the data, values of objective attributes should be there for utilization. Hence completeness is inextricably linked with data quality. For implementation, in a dataset, the completeness can be demonstrated as the proportion of values that are stored to the number of possible records of the dataset. Or by Pipino, Lee, and Wang's definition [2], it is \"a function of the missing values in a column of a table\".\n",
    "#### Comparition of the two Completeness Formulas\n",
    "I can compare the two formulas in terms of the difference between the representations of the completeness values. The row completeness demonstrates the completeness of values accross multiple columns, while the column completeness demonstrate the completeness of values in  columns. Hence the row completeness formula can show the degree of completeness over the entire dataset, while the column completeness formula shows the degree of completeness in specific columns.For this dataset, because only three columns may have missing values, calculating the row completeness formula regarding combinations of columns are easy to implement, and the values calculated can provide a more thorough overview of the completeness than the ones calculated column completeness formula. We can obtain the overview of the rate of any values in these columns are missing as well as the inspect of a specific combination of columns that all the values inside them are missing. In the column completeness calculation, we can only inspect the value for an individual column.\n",
    "\n",
    "[1] Li, C.: Computing complete answers to queries in the presence of limited access patterns. VLDB J. 12, 211–277 (2003)\\\n",
    "[2] Pipino, L.L., Lee, Y.W., Wang, R.Y.: Data quality assessment. Commun. ACM 45, 211–218 (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3576f",
   "metadata": {},
   "source": [
    "## Task7.2\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c313bd8b",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71c522",
   "metadata": {},
   "source": [
    "I use **Microsoft Excel** as the tool for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb4509",
   "metadata": {},
   "source": [
    "1. Open the original dataset in Excel.\n",
    "![img](./images/img.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f9008",
   "metadata": {},
   "source": [
    "2. For the entire column of \"Flag Text\", apply the function\n",
    "```excel\n",
    "=IF(ISBLANK(A2), \"\", TEXT(A2, \"dddd\"))\n",
    "```\n",
    ", which calculates days of the week regarding the \"Date\" column.\n",
    "![img](./images/img1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eede27",
   "metadata": {},
   "source": [
    "3. Then we want to add a new column \"Time\" that stores the corresponding time in the \"Date\" column, apply this function\n",
    "```excel\n",
    "=IF(ISBLANK(A2), \"\", TIME(HOUR(A2), MINUTE(A2), SECOND(A2)))\n",
    "```\n",
    "to the column to extract hour, minute, and second from the corresponding date. Then, update the type of the column from default \"General\" to \"Time\".\n",
    "![img](./images/img2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa59110",
   "metadata": {},
   "source": [
    "4. Select the entire table, press <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>L</kbd> to initialize the filters. Then we can apply filtering to the table.\n",
    "![img](./images/img3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d8c6d",
   "metadata": {},
   "source": [
    "5. Filter the sheet that the time is between 17:00 and 18:00, the direction is 1, and the day of the week is Friday.\\\n",
    "Customly filter the 'Time' to be between 17:00 and 18:00.\n",
    "![img](./images/img4.jpg)\n",
    "Filter the 'Direction Name' as 'North'.\n",
    "![img](./images/img5.jpg)\n",
    "Filter the 'Flag Text' as Friday.\n",
    "![img](./images/img6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dde777",
   "metadata": {},
   "source": [
    "6. Then, filter the 'Lane Name' to 'NB_MID'.\n",
    "![img](./images/img7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41a936",
   "metadata": {},
   "source": [
    "7. Create a sheet named 'manipulation' for storing the results of calculating the average speeds and other results. Create a column named 'Avg Speed Calculator' as the label of calculating function\n",
    "```excel\n",
    "=SUBTOTAL(101, task7.2!F:F)\n",
    "```\n",
    "to calculate the average speed for 'NB_MID' according to the filtering. Note that the first arguement is the code of average function with ignoring hidden values, we use this one because filtering hide rows.\\\n",
    "\\\n",
    "And we also add two columns 'Avg Speed (mph)' and 'Lane Name' for storing the average speeds for lanes and the lane names, respectively. Then we execute the function, copy and paste the value by plain text to the corresponding place in the column 'Avg Speed (mph)'. We do this because the result of this function changes according to filtering.\n",
    "![img](./images/img8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c814bf",
   "metadata": {},
   "source": [
    "8. Now repeat step 6, 7 to get the average speeds of all lanes.\n",
    "![img](./images/img9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e6ab9",
   "metadata": {},
   "source": [
    "9. Load the dataset of site 1415 in a newly created sheet named `task7.2 1415` and repeat steps 1 to 8 for this sheet. After all steps are finished, we can acquire the average values of the five north lanes.\n",
    "![img](./images/img10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be8dfe",
   "metadata": {},
   "source": [
    "10. Now we can calculate the average time with the given distance and the speed with the function\n",
    "```excel\n",
    "=3.019864/D2*60\n",
    "```\n",
    ".\n",
    "![img](./images/img11.jpg)\n",
    "Finally, we can compute the average over the average time by the function\n",
    "```excel\n",
    "=AVERAGE(E2:E6)\n",
    "```\n",
    ".\n",
    "![img](./images/img12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5077115",
   "metadata": {},
   "source": [
    "### Comparition of the Two Technologies\n",
    "Both of the two technologies have the ability to solve the tasks, they both have built-in functions for the same purposes in terms of data manipulation. And they can both manipulate the tabular dataset by filtering according to columns and aggregating according to rows.\n",
    "\n",
    "To discuss about the differences, the manipulation logics are different. For python, the manipulation is realized by codes. For Excel, the manipulation is realized by the combination of tools in the interface, such as the tool for applying filters to the 'Lane Name' column, and the built-in functions. Python codes can be compiled statically while every operation in Excel is executed instantaneously.\n",
    "\n",
    "For python, one advantage is that the process of manipulation is more interpretable and reviewable for a programmer. And using Jupyter Notebook to program this coursework can help me to divide operations into stages and add explanation everywhere I want. For example, for every task, I can split my manipulation operations in different code blocks by mini goals. Another advantage can be that the library pandas imported in python has efficient built-in functions such as `loc` that can automatically optimize the data manipulation operations. But the disadvantage is that bugs can possibly occur everywhere in my codes and the intermediate dataset cannot be entirely displayed during the manipulation.\n",
    "\n",
    "For Excel, the advantage is that the interface is intuitive and can display the data in a proper tabular structure and the drawing of charts is also very convenient. And for manipulating an entire column, Excel can help me with it with its tools conveniently rather than coding for loops manually. But the disadvantages are also significant. Because most of my manipulations are functions, their dependencies result in everytime I change a part of the Excel file, all the functions are re-calculated, which is time consuming. Another disadvantage can be that using tools in the interface and built-in functions in combination can bring incompatibility in some cases. For example, for calculating the average speed for different lanes, I have to manually filter the table for several times and then record the values manually. When the dataset become larger and more complex, each operation executed in Excel can take much longer time due to the deep function dependencies and large quantity of calculations. And manually manipulate some variables can be cumbersome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
